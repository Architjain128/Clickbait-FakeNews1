{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error,accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../../dataset/fakenews_dataset/300dims_vectorized_spacy20k.json','r')\n",
    "data = json.load(f)\n",
    "p=[]\n",
    "wX=[]\n",
    "woX=[]\n",
    "X1=[]\n",
    "X2=[]\n",
    "X3=[]\n",
    "Y=[]\n",
    "ID=[]\n",
    "i=0\n",
    "for x in data:\n",
    "    ID.append(x)\n",
    "    if(len(data[x])!=4):\n",
    "        print(x)\n",
    "    Y.append(data[x][0])\n",
    "    X1.append([data[x][1]])\n",
    "    X2.append(data[x][2])\n",
    "    X3.append(data[x][3])\n",
    "\n",
    "X1=(np.array(X1))\n",
    "X2=np.array(X2)\n",
    "X3=np.array(X3)\n",
    "wX=np.concatenate((X1,X2,X3),axis=1)\n",
    "woX=np.concatenate((X2,X3),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With clickbait score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_size = 0.2\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(wX, Y, test_size=test_size, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(multi_class='multinomial', random_state=0,max_iter=1000)\n",
    "lr.fit(X_train_std, Y_train)\n",
    "xgbc_params = {'n_estimators': 1000,'max_depth': 10,'learning_rate': 0.01,'booster': 'dart', 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.1, 'reg_lambda': 1}\n",
    "xgbc = XGBClassifier(objective='reg:squarederror',**xgbc_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more estimators to the ensemble\n",
    "estimators = [\n",
    "    ('xgbc',xgbc)    \n",
    "]\n",
    "reg = StackingClassifier(estimators=estimators,final_estimator=LogisticRegression(),n_jobs=-1)\n",
    "reg.fit(X_train, Y_train).score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y_test, reg.predict(X_test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_test, reg.predict(X_test_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of pkl name file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(reg, 'stkwc_1.pkl')\n",
    "saved_model = joblib.load('stkwc_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without clickbait score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_size = 0.2\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(woX, Y, test_size=test_size, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train, Y_train).score(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y_test, reg.predict(X_test_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_test, reg.predict(X_test_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of pkl name file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(reg, 'stkwoc_1.pkl')\n",
    "saved_model = joblib.load('stkwoc_1.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
